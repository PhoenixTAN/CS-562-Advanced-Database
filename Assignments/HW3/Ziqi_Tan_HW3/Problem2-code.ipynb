{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find 35 files.\n",
      "Find 6 queries.\n",
      "queries: \n",
      "['music play compos', 'ancient univers island', 'patient friend', 'loan pay money', 'work wealth fun', 'cold rain fog']\n",
      "preverse first 25 eigenvectors\n",
      "preserve 82.0% information\n",
      "Query by Matrix A1:\n",
      "[[27, 25, 26], [20, 2, 4], [12, 15, 6], [23, 14, 19], [1, 7, 27], [33, 7, 28]]\n",
      "preverse first 27 eigenvectors\n",
      "preserve 80.0% information\n",
      "Query by Matrix TF-IDF: \n",
      "[[27, 25, 26], [20, 35, 2], [15, 12, 6], [23, 3, 14], [32, 27, 7], [34, 31, 33]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import heapq\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dir = './Problem2-data/'\n",
    "stop_words_dir = dir + 'stop-words.txt'\n",
    "data_dir = dir + 'dataset/files/'\n",
    "query_dir = dir + 'dataset/queries/'\n",
    "\n",
    "d = 25    # preserve the first k eigenvectors\n",
    "top = 3   # find the top 3 similiarity\n",
    "\n",
    "\n",
    "def get_stop_words():\n",
    "    stop_words = []\n",
    "    stop_words_file = open(stop_words_dir)\n",
    "    file_content = stop_words_file.read()\n",
    "    stop_words = file_content.split(',')\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def remove_stop_words(stop_words, text):\n",
    "    text = text.split(' ')\n",
    "    new_words = []\n",
    "    for word in text:\n",
    "        word = word.strip()\n",
    "        if word not in stop_words:\n",
    "            new_words.append(word)\n",
    "    \n",
    "    new_text = ' '.join(new_words)\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "\n",
    "def stemmerize_text(text):\n",
    "    stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "    \n",
    "    text = text.split(' ')\n",
    "    for i in range(len(text)):\n",
    "        word = text[i]\n",
    "        word = word.strip()\n",
    "        stem_word = stemmer.stem(word)\n",
    "        text[i] = stem_word\n",
    "    \n",
    "    new_text = ' '.join(text)\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "\n",
    "def text_preprocess(text):\n",
    "    text = text.strip()\n",
    "    text = text.replace('\\n', ' ')\n",
    "    \n",
    "    # to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    punc_free = []\n",
    "    for ch in text:\n",
    "        if ch not in string.punctuation:\n",
    "            punc_free.append(ch)\n",
    "        else:\n",
    "            punc_free.append(' ')\n",
    "    new_text = ''.join(punc_free)    \n",
    "        \n",
    "    # replace number\n",
    "    # text = text.translate(text.maketrans('', '', string.digits))\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    texts = []\n",
    "    num_files = len([f for f in os.listdir(data_dir)if os.path.isfile(os.path.join(data_dir, f))]) \n",
    "    print('Find ' + str(num_files) + ' files.')\n",
    "    for i in range(num_files):\n",
    "        filepath = data_dir + 'file-' + str(i+1) + '.txt'\n",
    "        texts.append(open(filepath).read())\n",
    "    \n",
    "    return texts\n",
    "    \n",
    "\n",
    "def get_clean_text(texts):\n",
    "    '''\n",
    "        texts: a list of strings\n",
    "    '''\n",
    "    \n",
    "    stop_words = get_stop_words()\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        # preprocess data\n",
    "        texts[i] = text_preprocess(texts[i])\n",
    "\n",
    "        # remove stop words       \n",
    "        texts[i] = remove_stop_words(stop_words, texts[i])\n",
    "\n",
    "        # stemmerize\n",
    "        texts[i] = stemmerize_text(texts[i])\n",
    "    \n",
    "    return texts\n",
    "\n",
    "\n",
    "def load_query():\n",
    "    \n",
    "    queries = []\n",
    "    num_queries = len([f for f in os.listdir(query_dir)if os.path.isfile(os.path.join(query_dir, f))]) \n",
    "    print('Find ' + str(num_queries) + ' queries.')\n",
    "    for i in range(num_queries):\n",
    "        filepath = query_dir + 'query-' + str(i+1) + '.txt'\n",
    "        queries.append(open(filepath).read())\n",
    "        \n",
    "    return queries\n",
    "\n",
    "\n",
    "def map_query(S, u, v, query_v):\n",
    "    S = np.linalg.inv(S)\n",
    "    query_m = []\n",
    "    for q in query_v:\n",
    "        q_m = np.dot(np.dot(q, u), S)\n",
    "        query_m.append(q_m)\n",
    "    return query_m\n",
    "\n",
    "\n",
    "def cosine(q, d):\n",
    "    '''\n",
    "        consine similiarity\n",
    "    '''\n",
    "    # normalize the vectors\n",
    "    denominator = np.linalg.norm(q) * np.linalg.norm(d)\n",
    "    numerator = np.sum(np.multiply(q, d))\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "def svd_decomposition(matrix):\n",
    "\n",
    "    u, s, v = np.linalg.svd(matrix)\n",
    "    # u (3842, 3842)\n",
    "    # s (35,)\n",
    "    # v (35, 35)\n",
    "    u = u[:, :d]\n",
    "    \n",
    "    S = np.zeros((d, d))\n",
    "    eigenvalues = 0\n",
    "    for i in range(d):\n",
    "        S[i][i] = s[i]\n",
    "        eigenvalues += s[i]\n",
    "        \n",
    "    print('preverse first ' + str(d) + ' eigenvectors')\n",
    "    print('preserve ' + str(np.round(eigenvalues/np.sum(s)*100)) + '%' + ' information')\n",
    "    \n",
    "    v = v[:d, :]\n",
    "\n",
    "    return S, u, v\n",
    "\n",
    "\n",
    "def query(text, queries, matrix_type='counter'):\n",
    "    \n",
    "    vectorizer = None\n",
    "    if matrix_type == 'counter':\n",
    "        vectorizer = CountVectorizer()\n",
    "    elif matrix_type == 'tf-idf':\n",
    "        vectorizer = TfidfVectorizer()\n",
    "    \n",
    "    # TD Matrix \n",
    "    X = vectorizer.fit_transform(text)  \n",
    "    TD_matrix = np.array(X.toarray()).T\n",
    "    \n",
    "    # list of words\n",
    "    dictionary = vectorizer.get_feature_names()\n",
    "    \n",
    "    # vectorize queries\n",
    "    query_v = [vectorizer.transform([q]).toarray() for q in queries]\n",
    "\n",
    "    # SVD\n",
    "    S, u, v = svd_decomposition(TD_matrix)\n",
    "\n",
    "    # map query to new feature space\n",
    "    query_m = map_query(S, u, v, query_v)\n",
    "\n",
    "    resultSet = []\n",
    "    for q in query_m:\n",
    "        similarity = []\n",
    "        for d in v.T:\n",
    "            similarity.append(cosine(q, d))\n",
    "        topk_index = list(map(similarity.index, heapq.nlargest(top, similarity))) \n",
    "        topk_index = [index + 1 for index in topk_index]\n",
    "        resultSet.append(topk_index)\n",
    "\n",
    "    return TD_matrix, dictionary, resultSet\n",
    "\n",
    "\n",
    "def main():\n",
    "    global d\n",
    "    texts = load_data()\n",
    "    clean_text = get_clean_text(texts)\n",
    "    \n",
    "    queries = load_query()\n",
    "    queries = get_clean_text(queries)\n",
    "    print('queries: ')\n",
    "    print(queries)\n",
    "      \n",
    "    # Count\n",
    "    TD_matrix, dictionary, resultSet = query(clean_text, queries, matrix_type='counter')\n",
    "    print('Query by Matrix A1:')\n",
    "    print(resultSet)\n",
    "    \n",
    "    # print the matrix A1\n",
    "    TD = pd.DataFrame(data=TD_matrix, index=dictionary, columns=range(1, len(texts)+1))\n",
    "    TD.to_csv('TD-Matrix.csv')\n",
    "    \n",
    "    \n",
    "    # TF-IDF\n",
    "    \n",
    "    d = 27\n",
    "    TD_matrix, dictionary, resultSet = query(clean_text, queries, matrix_type='tf-idf')\n",
    "    print('Query by Matrix TF-IDF: ')\n",
    "    print(resultSet)\n",
    "\n",
    "    TD_TFIDF = pd.DataFrame(data=TD_matrix, index=dictionary, columns=range(1, len(texts)+1))\n",
    "    TD_TFIDF.to_csv('TFIDF-Matrix.csv')\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "main() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
